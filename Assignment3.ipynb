{"cells": [{"cell_type": "code", "execution_count": 7, "id": "bad55148-1c57-4ccb-915b-42b315d976c8", "metadata": {}, "outputs": [], "source": "# Part 1: Data Exploration with Apache Spark (10%) \n#Data Loading and Schema Understanding (2%) Load the dataset into a Spark DataFrame. \n#Print the schema and verify the data types of each column\n#starting spark session\nfrom pyspark.sql import SparkSession\nspark =SparkSession.builder.appName(\"spark-spotify\").getOrCreate()"}, {"cell_type": "code", "execution_count": 8, "id": "d9ca3ab8-a5b9-449d-86c8-2391b3146a24", "metadata": {}, "outputs": [], "source": "#loading the data\n#data is in the bucket\npath = 'gs://hive-buckets/spotify_songs.csv'\nfile_type = \"csv\"\n\n#csv file\ninfer_schema = 'true'\nfirst_row_is_header = 'true'\ndelimiter = ','\n\n#import csv\ndf= spark.read.format(file_type)\\\n    .option(\"inferSchema\",infer_schema)\\\n    .option(\"header\",first_row_is_header)\\\n    .option(\"sep\",delimiter)\\\n    .load(path)"}, {"cell_type": "code", "execution_count": 9, "id": "52a684f6-d92f-43bb-ba0f-296733be1c05", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "total number of rows in spotify data: 32833\n"}], "source": "#total number of rows in spotify data\ntotal_rows=df.count()\nprint(\"total number of rows in spotify data:\",total_rows) "}, {"cell_type": "code", "execution_count": 10, "id": "f358bbd1-13be-438f-9e2c-4fafe3230485", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- track_id: string (nullable = true)\n |-- track_name: string (nullable = true)\n |-- track_artist: string (nullable = true)\n |-- track_popularity: integer (nullable = true)\n |-- track_album_id: string (nullable = true)\n |-- track_album_name: string (nullable = true)\n |-- track_album_release_date: string (nullable = true)\n |-- playlist_name: string (nullable = true)\n |-- playlist_id: string (nullable = true)\n |-- playlist_genre: string (nullable = true)\n |-- playlist_subgenre: string (nullable = true)\n |-- danceability: string (nullable = true)\n |-- energy: string (nullable = true)\n |-- key: string (nullable = true)\n |-- loudness: string (nullable = true)\n |-- mode: string (nullable = true)\n |-- speechiness: double (nullable = true)\n |-- acousticness: double (nullable = true)\n |-- instrumentalness: double (nullable = true)\n |-- liveness: double (nullable = true)\n |-- valence: double (nullable = true)\n |-- tempo: double (nullable = true)\n |-- duration_ms: double (nullable = true)\n\n"}], "source": "df.printSchema()"}, {"cell_type": "code", "execution_count": 38, "id": "06701861-1ff5-43be-ae92-671bf6cc92b3", "metadata": {}, "outputs": [{"data": {"text/plain": "Row(track_id='6f807x0ima9a1j3VPbc7VN', track_name=\"I Don't Care (with Justin Bieber) - Loud Luxury Remix\", track_artist='Ed Sheeran', track_popularity=66, track_album_id='2oCs0DGTsRO98Gh5ZSl2Cx', track_album_name=\"I Don't Care (with Justin Bieber) [Loud Luxury Remix]\", track_album_release_date='2019-06-14', playlist_name='Pop Remix', playlist_id='37i9dQZF1DXcZDD7cfEKhW', playlist_genre='pop', playlist_subgenre='dance pop', danceability=0.748, energy=0.916, key='6', loudness='-2.634', mode='1', speechiness=0.0583, acousticness=0.102, instrumentalness=0.0, liveness=0.0653, valence=0.518, tempo=122.036, duration_ms=194754.0)"}, "execution_count": 38, "metadata": {}, "output_type": "execute_result"}], "source": "df.head() #grab the first row of your data"}, {"cell_type": "code", "execution_count": 45, "id": "eb1aaf03-d754-431c-aec5-b0c733ba58ce", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+------------------+------------------+------------------+\n| avg(danceability)|       avg(energy)|        avg(tempo)|\n+------------------+------------------+------------------+\n|0.6548413911824773|0.6987159026254852|120.82878928517026|\n+------------------+------------------+------------------+\n\n"}], "source": "#2.\n#Data Aggregation (3%)\n\n#Calculate the average danceability, energy, and tempo of tracks by artist.\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import avg\nfrom pyspark.sql.types import DoubleType\ndf = df.withColumn(\"danceability\", df[\"danceability\"].cast(DoubleType()))\ndf = df.withColumn(\"energy\", df[\"energy\"].cast(DoubleType()))\n#average_ = (df.groupBy(\"track_artist\").avg(\"danceability\",\"energy\", \"tempo\"))\naverage_D_E_Tstats = (df.agg(avg(\"danceability\"), avg(\"energy\"), avg(\"tempo\")))\naverage_D_E_Tstats.show()"}, {"cell_type": "code", "execution_count": 63, "id": "0a56ecbb-0cb0-451a-b67b-4b80b55d7cd6", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 35:>                                                         (0 + 2) / 2]\r"}, {"name": "stdout", "output_type": "stream", "text": "+-------------+---------------------+\n| track_artist|avg(track_popularity)|\n+-------------+---------------------+\n|Trevor Daniel|                 97.0|\n|          Y2K|                 91.0|\n|  Don Toliver|    90.71428571428571|\n|  Roddy Ricch|    88.21052631578948|\n|       DaBaby|    87.85714285714286|\n+-------------+---------------------+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "#Identify the top 5 artists with the highest average track popularity\nfrom pyspark.sql.functions import avg, col\navg_pop = (\n    df.groupBy(\"track_artist\")\n    .agg(avg(\"track_popularity\"))\n)\ntop5 = avg_pop.orderBy(col(\"avg(track_popularity)\").desc()).limit(5)\ntop5.show()"}, {"cell_type": "code", "execution_count": null, "id": "d7ff937d-0a2e-4710-9d5e-1c9927b68a38", "metadata": {}, "outputs": [], "source": "#3. Data Transformation (3%)\u2022 Create a new column called \u201cenergy_level\u201d that classifies tracks as'High Energy' (energy > 0.8) or 'Regular Energy' (energy \u2264 0.8)\n#Group the data by this new energy classification and calculate theaverage popularity and loudness for each energy_level"}, {"cell_type": "code", "execution_count": 74, "id": "8738cef6-e038-4ad7-87a3-8233132575cb", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 38:>                                                         (0 + 2) / 2]\r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------+---------------------+------------------+\n|  energy_level|avg(track_popularity)|     avg(loudness)|\n+--------------+---------------------+------------------+\n|   High Energy|    38.10813030385984|-4.875180217173095|\n|Regular Energy|    44.66595044344884|-7.636742925067416|\n+--------------+---------------------+------------------+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "from pyspark.sql.functions import when,avg, col\ndf = df.withColumn(\"energy_level\", when(col(\"energy\") > 0.8, \"High Energy\").otherwise(\"Regular Energy\"))\ngroup_energy_level = (df.groupBy(\"energy_level\").agg(avg(\"track_popularity\"), avg(\"loudness\")))\ngroup_energy_level.show()"}, {"cell_type": "code", "execution_count": 77, "id": "609e6274-c415-4ea0-8b6f-06dadfde4ed0", "metadata": {}, "outputs": [{"ename": "AnalysisException", "evalue": "path hdfs://cluster-nov-m/path/to/high_energy already exists.", "output_type": "error", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)", "\u001b[0;32m/tmp/ipykernel_10996/3782162098.py\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mhigh_energy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"energy_level\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"High Energy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mhigh_energy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mhigh_energy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/path/to/high_energy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[1;32m   1238\u001b[0m             \u001b[0mlineSep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlineSep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m         )\n\u001b[0;32m-> 1240\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m     def orc(\n", "\u001b[0;32m/opt/conda/miniconda3/lib/python3.10/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;31mAnalysisException\u001b[0m: path hdfs://cluster-nov-m/path/to/high_energy already exists."]}], "source": "# Data Exporting (2%)\u2022 Export the data that have been classified as 'High Energy'\nfrom pyspark.sql.functions import when,avg, col\nhigh_energy=df.filter(col(\"energy_level\") == \"High Energy\")\nhigh_energy.show\nhigh_energy.write.csv(\"/path/to/high_energy\", header=True)"}, {"cell_type": "code", "execution_count": null, "id": "b4608671-75c6-4da1-a468-43a58527a0cc", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.8"}}, "nbformat": 4, "nbformat_minor": 5}